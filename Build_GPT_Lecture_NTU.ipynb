{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/aisingapore/learning-buildgpt2/blob/main/Build_GPT_Lecture_NTU.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Requirements\n",
    "!pip install requests dotenv datasets 2>/dev/null || true\n",
    "!pip install transformers tensorboard tiktoken 2>/dev/null || true\n",
    "!pip install matplotlib seaborn 2>/dev/null || true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crSg6nd8pSMD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **Building GPT-2 From Scratch (To Undertand How LLMs Work)**\n",
    "Updated: 7 February 2025 version 1.0\n",
    "\n",
    "## **Introduction**\n",
    "This class walks through the process of building a transfomer-based large language model from scratch (i.e. GPT-2) to give students a feel for what is involved and to be understand some key concepts. We will use a combination of lecture, code, and practice examples in this class.\n",
    "\n",
    "**Instructors:**\n",
    "* Dr Leslie Teo, Senior Director, AI Products, AI Singapore\n",
    "* Dr William Thji, Head, Applied Research in Foundation Models, AI Products, AI Singapore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Prerequisites:**\n",
    "* Deep Learning fundamentals\n",
    "* PyTorch experience\n",
    "* Basic NLP concepts\n",
    "* Access to colab\n",
    "\n",
    "**Duration:**\n",
    "* 3 hours, with additional project work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaPshQKetlmp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **Figure 0: Overview of Lecture**\n",
    "\n",
    "![Figure 0.](images/Figure%200.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lENJa8HVQmh",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Part 1: Introduction to Large Language Models (LLMs)**\n",
    "\n",
    "**Learning Objectives:**\n",
    "* Understand what Large Language Models (LLMs) are and their capabilities.\n",
    "* Explore common use cases and applications of LLMs.\n",
    "* Learn about the brief history of LLMs.\n",
    "* Understand why we study GPT-2.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1evnpu9t2dd7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **1.1 What are LLMs?**\n",
    "\n",
    "**Definition and Capabilities:**\n",
    "\n",
    "   *   **Large Language Models (LLMs)** are deep learning models trained on massive datasets of text and code. They are designed to **understand and generate human-like text**, **translate languages**, **write different kinds of creative content**, and **answer your questions in an informative way**, even if they are open ended, challenging, or strange. Today LLMs also encompass speech, music (audio), images, and video.\n",
    "   *   They learn **statistical patterns and relationships** between words, phrases, and sentences, enabling them to perform a wide range of language-based tasks.\n",
    "   *   **Key characteristics:**\n",
    "        *   **Large:** These models have billions, even trillions, of parameters (weights and biases).\n",
    "        *   **Language-focused:** They are specifically trained for understanding and generating natural language.\n",
    "        *   **Pre-trained:** They are first trained on a vast corpus of general text data and can then be fine-tuned for specific tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **Figure 1. AI and LLMs**\n",
    "\n",
    "![Figure 1](images/Figure%201.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Examples:**\n",
    "\n",
    "   *   GPT-3, GPT-4 (OpenAI) - These are later, more powerful versions.\n",
    "   *   BERT (Google) -  A different type of Transformer (encoder-only).\n",
    "   *   LaMDA, PaLM (Google) - Other large-scale models.\n",
    "   *   BLOOM (BigScience) - An open-source multilingual LLM.\n",
    "   *   LLaMA (Meta) - Another powerful family of LLMs.\n",
    "   *   Qwen (Alibaba) - Representing Chinese Big Tech.\n",
    "   *   Deepseek (Deepseek) - Powerful open weights models.\n",
    "   *   ChatGPT, Claude, Gemini: Popular applications built on top of LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Generative Nature of LLMs**\n",
    "\n",
    "   *   LLMs are considered \"generative\" because they **create new text sequences** that are not simply copied from their training data.\n",
    "   *   They generate text by **sampling from a probability distribution over possible word sequences**. At each step, the model predicts the probability of the next word, given the preceding words, and then a word is selected (sampled) based on these probabilities.\n",
    "   *   This probabilistic nature allows LLMs to produce diverse and creative outputs.\n",
    "\n",
    "**Why study GPT-2 if we have newer, bigger models?**\n",
    "\n",
    "*   **Foundational Understanding:** GPT-2 is a great model for learning the **fundamentals of transformers and LLMs**. Its architecture is simpler than the later models, making it easier to dissect and understand.\n",
    "*   **Accessibility:** The model and its pre-trained weights are readily available through libraries like Hugging Face Transformers.\n",
    "*   **Resource-Friendly:** You can experiment with and even fine-tune GPT-2 on less powerful hardware compared to the behemoths like GPT-3 or GPT-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Key Concept:** LLMs are powerful deep learning models that can understand and generate human-like text by learning statistical patterns from vast amounts of data. They are **generative** because they create new text sequences by sampling from learned probability distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Using SEA-LION\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access your variables\n",
    "API_KEY = os.getenv('API_KEY')\n",
    "\n",
    "url = \"https://api.sea-lion.ai/v1/chat/completions\"\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer \" + API_KEY,\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hi SEA-LION\"}],\n",
    "    \"model\": \"aisingapore/llama3.1-8b-cpt-sea-lionv3-instruct\",\n",
    "    \"stream\": False,\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Km6tWKWmWRhN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### **1.2 Use Cases and Applications of LLMs**\n",
    "\n",
    "**Common Applications:**\n",
    "   *   **Proprietary Services:** Using pre-built and hosted LLM services like **ChatGPT** for various tasks (e.g., writing assistance, question answering, code generation).\n",
    "   *   **Custom Deployments:** Running your own instance of an LLM (like Llama) locally on your own hardware. This gives you more control over the model, data privacy, and customization.\n",
    "   *   **API Integration:** Deploying LLMs as API endpoints that other applications can interact with. This allows you to integrate LLM capabilities into your own software systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Examples**\n",
    "\n",
    "*   **Chatbots and Conversational AI:** Creating intelligent agents that can engage in natural language conversations.\n",
    "*   **Text Generation:** Writing stories, articles, scripts, poems, code, and other forms of creative content.\n",
    "*   **Machine Translation:** Translating text from one language to another.\n",
    "*   **Question Answering:** Providing answers to questions based on a given context or general knowledge.\n",
    "*   **Text Summarization:** Condensing large amounts of text into shorter, concise summaries.\n",
    "*   **Code Completion and Generation:** Assisting developers by suggesting code completions and even generating entire code blocks.\n",
    "*   **Sentiment Analysis:** Determining the emotional tone or sentiment expressed in a piece of text.\n",
    "*   **And many more emerging applications...**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Key Concept:** LLMs have a wide range of applications, from using readily available services like ChatGPT to building custom solutions and integrating LLMs into various software systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zi9DF8FXAzW",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "### **1.3 A Brief History of LLMs**\n",
    "\n",
    "*   **Early Days (Pre-2017):**\n",
    "    *   **Statistical Language Models (n-gram models):** These models used statistical methods to predict the probability of a sequence of words. Limited by their inability to capture long-range dependencies.\n",
    "    *   **Recurrent Neural Networks (RNNs), specifically LSTMs:** RNNs, and especially LSTMs, became popular for language modeling because they could process sequential data and theoretically capture longer dependencies. However, they suffered from issues like vanishing gradients and difficulty in parallelization. Andrej Karpathy's blog post \"The Unreasonable Effectiveness of Recurrent Neural Networks\" demonstrated the power of character-level RNNs.\n",
    "*   **The Transformer Revolution (2017):**\n",
    "    *   The \"Attention is All You Need\" paper introduced the **Transformer** architecture. This was a paradigm shift. Transformers use a mechanism called \"attention\" that allows them to weigh the importance of different words in a sequence when making predictions, overcoming limitations of the RNN.\n",
    "    *   **Key Advantages of Transformers:**\n",
    "        *   **Parallelization:** Transformers can process the entire input sequence in parallel, significantly speeding up training.\n",
    "        *   **Long-Range Dependencies:** Attention allows them to effectively capture relationships between words that are far apart in a sequence.\n",
    "*   **The Rise of LLMs (2018 - Present):**\n",
    "    *   **BERT (2018):** Bidirectional Encoder Representations from Transformers. Introduced masked language modeling (MLM) during pre-training.\n",
    "    *   **GPT (2018), GPT-2 (2019), GPT-3 (2020), GPT-4 (2023):** OpenAI's Generative Pre-trained Transformer series. These models demonstrated the power of scaling up model size and training data.\n",
    "    *   **Other Notable LLMs:** Many other organizations followed suit, developing their own large models (e.g., LaMDA, PaLM, BLOOM, LLaMA).\n",
    "*   **The Current Landscape:**\n",
    "    *   Rapid progress and research continue, pushing the boundaries of model size, capabilities, and efficiency.\n",
    "    *   Ethical considerations, bias, and safety are increasingly important topics in the field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4Rqr6E0Wqco",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **1.4 Training an LLM - Let's Use GPT-2**\n",
    "\n",
    "*   **Overview of GPT-2**\n",
    "    *   GPT-2 is a **decoder-only Transformer model**. This means it's designed for text generation, predicting the next word in a sequence given the preceding words. In contrast, an encoder-decoder architecture, such as the one originally described in [Attention is All You Need](https://arxiv.org/abs/1706.03762), is more common in tasks like machine translation where the input and output can have different lengths.\n",
    "    *   It was developed by OpenAI and released in stages due to concerns about potential misuse.\n",
    "    *   It was a significant advancement in the field of NLP, demonstrating the power of scaling up model size and training data.\n",
    "*   **GPT-2 Miniseries and Scaling Laws**\n",
    "    *   OpenAI released different sizes of GPT-2 models (small, medium, large, XL), each with an increasing number of parameters.\n",
    "    *   **Scaling Laws:** Research on GPT-2 and other LLMs has shown that there are **predictable relationships** between model size, dataset size, compute used for training, and the resulting model performance. Generally, **larger models trained on more data with more compute tend to perform better**. This also informs why we are focusing on GPT-2 now, which can help us to learn some of the fundamental concepts.\n",
    "\n",
    "*   **Availability**\n",
    "    *   Pre-trained weights and code for GPT-2 are publicly available, primarily through libraries like **Hugging Face Transformers**. This makes it relatively easy to get started with using and experimenting with GPT-2.\n",
    "*   **Demonstration**\n",
    "    *   We will (later in the course) run a simplified GPT-2 training example and show the **objective (predicting the next word)** and how the **loss function decreases over time** as the model learns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpqYcGwf_RgG",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Prepare input with prompt\n",
    "prompt = \"GPT2 is a model developed by OpenAI. Tomorrow is Chinese New Year\"\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=100,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "# Generate text\n",
    "gen_tokens = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Decode and print the generated text\n",
    "gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDdRnCZJXU3o",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **1.5 Core Concepts: Transformers and Attention**\n",
    "\n",
    "*   **1.5.1 The Transformer Architecture:**\n",
    "    *   **Encoder-Decoder Structure (Original Transformer):**\n",
    "        *   The original Transformer had an **encoder** and a **decoder**.\n",
    "        *   **Encoder:** Processes the input sequence and creates a contextualized representation.\n",
    "        *   **Decoder:** Generates the output sequence based on the encoder's representation and its own previous outputs.\n",
    "        *   **Example:** Machine translation - the encoder processes the source language sentence, and the decoder generates the target language translation.\n",
    "    *   **GPT-2: Decoder-Only Model:**\n",
    "        *   GPT-2 uses **only the decoder** part of the Transformer. It's designed for text generation.\n",
    "        *   It predicts the next word in a sequence given the preceding words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Reference:** [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) by Jay Alammar\n",
    "\n",
    "\n",
    "### **Figure 2: Transformer Architecture (Encoder-Decoder)**\n",
    "\n",
    "![Figure 2a: Transformer](images/Figure%202a.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Figure 2b: Transformer](images/Figure%202b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Figure 3: GPT-2 Architecture (Decoder-Only)**\n",
    "\n",
    "\n",
    "![Figure 3: GPT-2](images/Figure%203.png)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Key Components:**\n",
    "*   **Embedding Layer:** Converts words into numerical vectors (**embeddings**). These vectors capture the semantic meaning of words.\n",
    "*   **Positional Encoding:** Adds information about the **position** of words in the sequence to the embeddings. This is crucial because, unlike RNNs, Transformers don't inherently process data sequentially.\n",
    "*   **Decoder Layers (GPT-2 has multiple):** Each layer consists of:\n",
    "*   **Masked Self-Attention:** Allows the model to attend to **preceding** words in the sequence when predicting the next word (more on this below). The \"masked\" part ensures that the model doesn't \"cheat\" by looking at future words during training.\n",
    "*   **Feed-Forward Neural Network:** A simple fully connected network that further processes the output of the attention layer.\n",
    "*  **Output Layer:** A linear layer followed by a softmax function that produces a **probability distribution over the vocabulary**, indicating the likelihood of each word being the next word. The model assigns a probability to every possible word in its vocabulary, indicating how likely it is to be the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*   **1.5.2 Attention Mechanism:**\n",
    "    *   **Intuition:** **Attention** allows the model to **focus on the most relevant parts of the input sequence** when making a prediction. Think of it like highlighting the important words in a sentence when you're trying to understand its meaning.\n",
    "    *   **Analogy:** Imagine you're translating a sentence. You don't just translate each word in isolation. You consider the entire sentence, paying more attention to certain words that are crucial for understanding the meaning of the word you're currently translating. This is how attention allows the model to \"weigh\" the importance of other words.\n",
    "    *   **Self-Attention:** In **self-attention**, the input sequence attends to **itself**. Each word in the sequence attends to every other word in the same sequence.\n",
    "    *   **Masked Self-Attention (in GPT-2):** In GPT-2, the self-attention is \"**masked**\" during training. This means that when a word is attending to other words, it can **only attend to the words that came before it** in the sequence. This is because GPT-2 is trained to predict the next word, and allowing it to see future words would be like giving it the answers during a test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**How it Works (Simplified):**\n",
    "  \n",
    "   * Scenario: We have the input sentence \"The cat sat on the mat\".  We want to understand how the attention mechanism works when focusing on the word \"sat\".\n",
    "   * Simplified Analogy: Imagine you're reading this sentence. When you get to \"sat\", you instinctively pay more attention to \"cat\" and \"mat\" because they tell you who is sitting and where.  Attention in Transformers works similarly.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "   * Queries, Keys, and Values: These are vector representations of the words.  Think of them as different facets of the word.  For our example, let's simplify and say each word has a \"meaning\" vector.\n",
    "        * Query: The word we're currently focusing on (\"sat\" in this case). It's asking \"What am I related to?\"\n",
    "        * Keys: The other words in the sentence (\"the\", \"cat\", \"on\", \"the\", \"mat\"). They represent \"What information do I offer?\"\n",
    "        * Values: Also the other words. They represent the actual information the word carries. (In reality, Keys and Values are often the same).\n",
    "    * Scoring: We calculate a \"score\" between the Query (\"sat\") and each Key (every word).  This score represents how much the other word relates to the current word. A higher score means more attention.  We can use a simple dot product for this example.\n",
    "    * Softmax: We apply a softmax function to the scores. This converts the scores into probabilities that sum up to 1. These probabilities represent the attention weights.\n",
    "    * Weighted Sum: We multiply each Value (the actual word representation) by its corresponding attention weight and sum them up.  This weighted sum is the output of the attention mechanism for the word \"sat\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example:**\n",
    "\n",
    "Let's pretend our word vectors are just single numbers (for simplicity).  In reality, they'd be much larger vectors.\n",
    "\n",
    "Word\tMeaning (Query/Key/Value)\n",
    "The\t1\n",
    "cat\t5\n",
    "sat\t3\n",
    "on\t2\n",
    "the\t1\n",
    "mat\t4\n",
    "\n",
    "Export to Sheets\n",
    "Query (sat): 3\n",
    "\n",
    "Keys (all words): 1, 5, 3, 2, 1, 4\n",
    "\n",
    "Scoring (Dot Product):\n",
    "\n",
    "\"sat\" x \"the\" = 3 * 1 = 3\n",
    "\"sat\" x \"cat\" = 3 * 5 = 15\n",
    "\"sat\" x \"sat\" = 3 * 3 = 9\n",
    "\"sat\" x \"on\" = 3 * 2 = 6\n",
    "\"sat\" x \"the\" = 3 * 1 = 3\n",
    "\"sat\" x \"mat\" = 3 * 4 = 12\n",
    "Softmax (Simplified): Let's just normalize these scores so they sum to 1 (a true softmax is a bit more complex).  The exact values aren't crucial for the illustration:\n",
    "\n",
    "\"the\": 3 / (3+15+9+6+3+12) ≈ 0.04\n",
    "\"cat\": 15 / (3+15+9+6+3+12) ≈ 0.24\n",
    "\"sat\": 9 / (3+15+9+6+3+12) ≈ 0.14\n",
    "\"on\": 6 / (3+15+9+6+3+12) ≈ 0.10\n",
    "\"the\": 3 / (3+15+9+6+3+12) ≈ 0.04\n",
    "\"mat\": 12 / (3+15+9+6+3+12) ≈ 0.19\n",
    "Weighted Sum:\n",
    "\n",
    "(0.04 * 1) + (0.24 * 5) + (0.14 * 3) + (0.10 * 2) + (0.04 * 1) + (0.19 * 4) ≈ 1.87\n",
    "Interpretation:\n",
    "\n",
    "The output for \"sat\" is approximately 1.87. Notice how \"cat\" and \"mat\" have the highest attention weights (0.24 and 0.19 respectively). This means the model paid more attention to \"cat\" and \"mat\" when processing \"sat\", which makes intuitive sense. The number 1.87 is a weighted representation that encodes the context around \"sat\".\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Scaled Dot-Product Attention:** This is the most common type of attention used in Transformers, including GPT-2. The attention scores are scaled down by the square root of the dimension of the key vectors. This helps to prevent the gradients from becoming too small during training.\n",
    "\n",
    "**Formula for Scaled Dot-Product Attention:**\n",
    "\n",
    "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$\n",
    "            \n",
    "where:\n",
    "*   $Q$ is the matrix of queries\n",
    "*   $K$ is the matrix of keys\n",
    "*   $V$ is the matrix of values\n",
    "*   $d_k$ is the dimension of the keys\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Figure 4: Self-Attention Mechanism**\n",
    "\n",
    "![Figure 4: High-level Attention](images/Figure%204.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Figure 5: Self-Attention Mechanism (Detailed)**\n",
    "\n",
    "\n",
    "![Figure 5: High-level Attention](images/Figure%205.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Multi-Head Attention:**\n",
    "\n",
    "   *   Instead of having just one set of Q, K, and V vectors per word, **multi-head attention** uses multiple sets (multiple \"**heads**\").\n",
    "   *   Each head learns to attend to **different aspects** of the input sequence.\n",
    "   *   **Analogy:** Think of it like having multiple experts examining the same sentence, each focusing on a different aspect (e.g., syntax, semantics, coreference).\n",
    "   *   The outputs of the multiple heads are concatenated and then passed through a linear layer to produce the final output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Figure 6: Multi-Head Attention**\n",
    "\n",
    "![Figure 6: Multi-Head Attention](images/Figure%206.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Attention examples\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        batch_size, seq_len, _ = keys.shape\n",
    "        query = query.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        combined = torch.cat([query, keys], dim=2)\n",
    "        scores = self.attention(combined).squeeze(2)\n",
    "        attention_weights = F.softmax(scores, dim=1)\n",
    "        output = torch.bmm(attention_weights.unsqueeze(1), keys).squeeze(1)\n",
    "        return output, attention_weights.unsqueeze(-1)  # Add dimension for visualization\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "        self.scale = math.sqrt(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        scores = torch.bmm(q, k.transpose(1, 2)) / self.scale\n",
    "        attention_weights = F.softmax(scores, dim=2)\n",
    "        output = torch.bmm(attention_weights, v)\n",
    "        return output, attention_weights\n",
    "\n",
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "        self.scale = math.sqrt(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        scores = torch.bmm(q, k.transpose(1, 2)) / self.scale\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "        mask = mask.unsqueeze(0).expand(batch_size, -1, -1).to(x.device)\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "        attention_weights = F.softmax(scores, dim=2)\n",
    "        output = torch.bmm(attention_weights, v)\n",
    "        return output, attention_weights\n",
    "\n",
    "def plot_attention_matrix(weights, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    weights_np = weights[0].detach().numpy()\n",
    "    if len(weights_np.shape) == 3:\n",
    "        weights_np = weights_np.squeeze()\n",
    "    sns.heatmap(weights_np, annot=True, fmt='.2f', cmap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.show()\n",
    "\n",
    "def run_demonstration():\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    hidden_size = 8\n",
    "\n",
    "    x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "    query = torch.randn(batch_size, hidden_size)\n",
    "\n",
    "    models = {\n",
    "        'Simple Attention': SimpleAttention(hidden_size),\n",
    "        'Self Attention': SelfAttention(hidden_size),\n",
    "        'Masked Self Attention': MaskedSelfAttention(hidden_size)\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, model in models.items():\n",
    "            if name == 'Simple Attention':\n",
    "                output, weights = model(query, x)\n",
    "            else:\n",
    "                output, weights = model(x)\n",
    "            plot_attention_matrix(weights, name)\n",
    "            print(f\"\\n{name} Shape:\", weights.shape)\n",
    "\n",
    "run_demonstration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6lnIlShJbat",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Part 2: Data Preparation and Tokenization**\n",
    "\n",
    "**Learning Objectives:**\n",
    "* Understand the importance of data preparation for LLMs.\n",
    "* Learn what tokenization is and why it's necessary.\n",
    "* Understand the different steps in the tokenization process.\n",
    "* Learn what Byte Pair Encoding is and how it works.\n",
    "* Be able to use tiktoken to encode and decode text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubXfgvauJIo6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **2.1 Importance of Data Preparation**\n",
    "\n",
    "*   **Data is the Foundation:** The performance and capabilities of an LLM are heavily dependent on the quality and nature of the data it's trained on.\n",
    "*   **Garbage In, Garbage Out:** If you train a model on poorly formatted, biased, or irrelevant data, you'll get a poorly performing, biased, or irrelevant model.\n",
    "*  **LLM Comprehension:** Data preparation, particularly **tokenization**, is crucial for how the LLM \"understands\" the input text. Tokenization allows the model to map words to numerical vectors, and the way you break down text into tokens directly affects what the model learns.\n",
    "*   **Key Considerations:**\n",
    "    *   **Data Source:** Where does the data come from? Is it representative of the kind of text you want the model to generate? (e.g. WebText for GPT-2, a collection of diverse web pages)\n",
    "    *   **Data Cleaning:** Removing noise, errors, inconsistencies, and irrelevant parts (e.g., HTML tags, special characters).\n",
    "    *   **Data Formatting:** Structuring the data in a way that the model can process (e.g., each training example might be a sequence of text of a certain length).\n",
    "    *   **Dataset Size:** Larger datasets generally lead to better performance, but there are diminishing returns.\n",
    "    *   **Data Bias:** Is the data biased in any way (e.g., gender, race, ideology)? This can lead to a biased model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Key Concept:** Data preparation is a critical step in building LLMs. The quality, format, and content of the training data directly impact the model's performance, capabilities, and potential biases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vGWJYWmKQHO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **2.2 Tokenization and Its Role**\n",
    "\n",
    "*   **What is Tokenization?**\n",
    "    *   Tokenization is the process of **splitting text into individual units called tokens**.\n",
    "    *   Tokens can be words, subwords, characters, or even punctuation marks, depending on the tokenization method.\n",
    "    *   Think of it like breaking down a sentence into its building blocks.\n",
    "\n",
    "*   **Why is it Necessary?**\n",
    "    *   **Numerical Representation:**  Neural networks work with numbers, not raw text. Tokenization is the first step in converting text into a numerical representation that the model can process.\n",
    "    *   **Vocabulary:** Tokenization defines the **vocabulary** of the model – the set of all unique tokens that the model recognizes.\n",
    "    *   **Input to the Model:** The sequence of tokens (represented as numerical IDs) becomes the input to the LLM.\n",
    "\n",
    "*   **Example:**\n",
    "\n",
    "    *   Sentence: \"The quick brown fox jumps over the lazy dog.\"\n",
    "    *   Tokenization (word-level): [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "**Key Concept:** Tokenization is the process of breaking down text into smaller units (tokens) that can be represented numerically, forming the model's vocabulary and providing the input for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECgjTK0AKdVc",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **2.3 Tokenization Process Detailed**\n",
    "\n",
    "*   **Steps Involved:**\n",
    "\n",
    "    1.  **Text Cleaning (Preprocessing):**\n",
    "        *   This step is often done *before* the core tokenization process.\n",
    "        *   It can involve:\n",
    "            *   Removing HTML tags or other markup.\n",
    "            *   Handling special characters or encoding issues.\n",
    "            *   Lowercasing text (optional, depends on the model and task).\n",
    "            *   Removing irrelevant sections (e.g., headers, footers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "   2.  **Text Splitting:**\n",
    "        *   The core of tokenization.\n",
    "        *   Text is split into tokens based on a set of rules or an algorithm.\n",
    "        *   Common approaches:\n",
    "            *   **Whitespace Splitting:** The simplest method, splitting on spaces and punctuation.\n",
    "                    *   **Rule-based Splitting:** Using predefined rules to handle special cases (e.g., contractions like \"don't\").\n",
    "            *   **Subword Tokenization (e.g., Byte Pair Encoding, WordPiece):** More advanced algorithms that split words into smaller units (subwords). We'll focus on Byte Pair Encoding (BPE) later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "   3.  **Token ID Conversion:**\n",
    "        *   Each unique token in the vocabulary is assigned a unique numerical ID.\n",
    "        *   A **vocabulary file** (or a mapping) stores the correspondence between tokens and their IDs.\n",
    "        *   Example:\n",
    "            *   \"The\": 0\n",
    "            *   \"quick\": 1\n",
    "            *   \"brown\": 2\n",
    "            *   ...and so on...\n",
    "\n",
    "   4.  **Embeddings Conversion (Conceptual Overview):**\n",
    "        *   This step happens *inside* the model, but it's important to understand the connection.\n",
    "        *   Token IDs are used to look up corresponding **word embeddings** in an embedding layer.\n",
    "        *   Embeddings are dense vector representations of words that capture semantic meaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*   **Example (Illustrative):**\n",
    "\n",
    "    ```\n",
    "    Text: \"Hello, world! This is a sentence.\"\n",
    "\n",
    "    1. Text Cleaning (e.g., lowercase, remove punctuation):\n",
    "       \"hello world this is a sentence\"\n",
    "\n",
    "    2. Text Splitting (e.g., whitespace splitting):\n",
    "       [\"hello\", \"world\", \"this\", \"is\", \"a\", \"sentence\"]\n",
    "\n",
    "    3. Token ID Conversion (using a hypothetical vocabulary):\n",
    "       [10, 25, 42, 18, 5, 67]\n",
    "\n",
    "    4. Embeddings Conversion (inside the model):\n",
    "       The model looks up the embedding vectors for each of these IDs in its embedding layer.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Key Concept:** Tokenization involves cleaning the text, splitting it into tokens, converting those tokens into numerical IDs based on a vocabulary, and then using these IDs to look up word embeddings inside the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GruskwcTKpgi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "### **2.4 Byte Pair Encoding (BPE)**\n",
    "\n",
    "*   **Introduction to BPE:**\n",
    "\n",
    "    *   BPE is a **subword tokenization algorithm** that is widely used in modern LLMs, including GPT-2.\n",
    "    *   It's a good balance between the granularity of character-level tokenization (which can result in very long sequences) and the limitations of word-level tokenization (which struggles with out-of-vocabulary words).\n",
    "    *   **Key Idea:** BPE iteratively merges the most frequent pair of consecutive tokens into a new, single token.\n",
    "\n",
    "*   **Handling Unknown Words:**\n",
    "\n",
    "    *   BPE can handle words that were not seen during training (out-of-vocabulary or OOV words) by breaking them down into subword units that are present in the vocabulary.\n",
    "    *   For example, if the word \"aaabdabc\" was not seen during training, but we have \"aaab\" and \"dabc\" in the vocabulary from above steps, we can tokenize it as two tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*   **How BPE Works:**\n",
    "\n",
    "    1.  **Start with a vocabulary of individual characters.**\n",
    "    2.  **Iteratively merge the most frequent pair of consecutive tokens** in the training data into a new token.\n",
    "    3.  **Add the new token to the vocabulary.**\n",
    "    4.  **Repeat step 2 and 3** until a desired vocabulary size is reached or a certain number of merges have been performed.\n",
    "\n",
    "\n",
    "*   **Example (Illustrative):**\n",
    "\n",
    "    Let's say we have the following training data (simplified):\n",
    "\n",
    "    ```\n",
    "    \"aaabdaaabac\"\n",
    "    ```\n",
    "\n",
    "    1.  **Initial Vocabulary:** `[\"a\", \"b\", \"c\", \"d\"]`\n",
    "\n",
    "    2.  **Iteration 1:**\n",
    "        *   Most frequent pair: \"aa\" (occurs twice)\n",
    "        *   New token: \"aa\"\n",
    "        *   Updated vocabulary: `[\"a\", \"b\", \"c\", \"d\", \"aa\"]`\n",
    "        *   Data: `[\"aa\", \"ab\", \"d\", \"aa\", \"ab\", \"a\", \"c\"]`\n",
    "\n",
    "    3.  **Iteration 2:**\n",
    "        *   Most frequent pair: \"ab\" (occurs twice)\n",
    "        *   New token: \"ab\"\n",
    "        *   Updated vocabulary: `[\"a\", \"b\", \"c\", \"d\", \"aa\", \"ab\"]`\n",
    "        *   Data: `[\"aa\", \"ab\", \"d\", \"aa\", \"ab\", \"a\", \"c\"]`\n",
    "\n",
    "    4.  **Iteration 3:**\n",
    "        *   Most frequent pair: \"aaab\" (occurs twice)\n",
    "        *   New token: \"aaab\"\n",
    "        *   Updated vocabulary: `[\"a\", \"b\", \"c\", \"d\", \"aa\", \"ab\", \"aaab\"]`\n",
    "        *   Data: `[\"aaab\", \"d\", \"aaab\", \"a\", \"c\"]`\n",
    "\n",
    "        ...and so on...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Key Concepts:**\n",
    "\n",
    "*   **BPE is a subword tokenization algorithm** that iteratively merges frequent token pairs.\n",
    "*   It helps to **balance vocabulary size and the ability to handle unknown words**.\n",
    "*   It's commonly used in modern LLMs like GPT-2.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCmQdDyOK7aj"
   },
   "source": [
    "\n",
    "### **2.5 Hands-on Exercise 1: Data Preparation and Tokenization**\n",
    "\n",
    "*   **Activity:** We'll use the `tiktoken` library, which provides a fast implementation of the BPE tokenizer used by GPT-2.\n",
    "\n",
    "*   **Task:**\n",
    "    1.  Take a sample text.\n",
    "    2.  Tokenize the text using the GPT-2 tokenizer from `tiktoken`.\n",
    "    3.  Inspect the generated tokens and their corresponding IDs.\n",
    "\n",
    "*   **Guidance:**\n",
    "\n",
    "    *   Observe how the text is split into tokens.\n",
    "    *   Notice how punctuation is handled.\n",
    "    *   See how subwords are used for words that might not be in the vocabulary (like \"blargh\" or \"pলাপ\").\n",
    "    *   Understand the relationship between tokens and their IDs.\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "*   Students should be able to use the `tiktoken` library to tokenize text using the GPT-2 tokenizer.\n",
    "*   They should understand how text is converted into a sequence of token IDs.\n",
    "*   They should observe how BPE handles subwords and potentially unknown words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdQav549Ltub",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## BPE example\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Get the GPT-2 tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a sample text for demonstrating GPT-2 tokenization. This includes some উদ্ভট words like pলাপ, and some made up ones like blargh, which should still tokenize properly.\"\n",
    "\n",
    "# Encode (tokenize) the text\n",
    "encoded_text = tokenizer.encode(text)\n",
    "\n",
    "# Decode (detokenize) the tokens back into text\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "\n",
    "# Print the encoded text (token IDs)\n",
    "print(\"Encoded text (token IDs):\", encoded_text)\n",
    "\n",
    "# Print the decoded text\n",
    "print(\"Decoded text:\", decoded_text)\n",
    "\n",
    "# Print each token and its ID\n",
    "print(\"Tokens and their IDs:\")\n",
    "for token_id in encoded_text:\n",
    "    print(f\"Token: '{tokenizer.decode([token_id])}', ID: {token_id}\")\n",
    "\n",
    "# Test with an out of vocabulary word\n",
    "oov_text = \"This is a text with an out of vocabulary word: blarghxyz\"\n",
    "encoded_oov_text = tokenizer.encode(oov_text)\n",
    "print(f\"Encoded OOV text: {encoded_oov_text}\")\n",
    "print(f\"Decoded OOV text: {tokenizer.decode(encoded_oov_text)}\")\n",
    "for token_id in encoded_oov_text:\n",
    "    print(f\"Token: '{tokenizer.decode([token_id])}', ID: {token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLsxBJjPNJHO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Part 3: Training GPT-2**\n",
    "\n",
    "**Learning Objectives:**\n",
    "* Understand the architecture of GPT-2\n",
    "* Learn how to implement a GPT-2 model from scratch\n",
    "* Understand what pre-training is and why it's important\n",
    "* Learn the core concepts of training\n",
    "* Run a simplified GPT-2 training example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azqhaCPNNPPD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **3.1 GPT-2 Architecture Overview**\n",
    "\n",
    "*   **Recap:** GPT-2 is a **decoder-only Transformer model**. It uses the decoder part of the original Transformer architecture (from \"Attention is All You Need\") for text generation.\n",
    "*   **Components:**\n",
    "    *   **Token Embeddings:**\n",
    "        *   Converts input tokens (represented as numerical IDs) into **dense vector representations** called embeddings.\n",
    "        *   These embeddings capture semantic meaning and relationships between words.\n",
    "        *   The embedding layer is essentially a large lookup table where each row corresponds to the embedding vector for a specific token ID in the vocabulary.\n",
    "    *   **Positional Encodings:**\n",
    "        *   Since Transformers don't inherently process sequential information, positional encodings are added to the token embeddings to provide information about the **position of each token in the sequence**.\n",
    "        *   These encodings can be learned or fixed (e.g., using sine and cosine functions, as in the original Transformer paper).\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " *   **Transformer Blocks (Decoder Layers):**\n",
    "        *   GPT-2 has multiple stacked Transformer blocks (or decoder layers). The number of blocks varies depending on the model size (e.g., 12 blocks in GPT-2 small, 48 in GPT-2 XL).\n",
    "        *   Each block consists of:\n",
    "            *   **Masked Self-Attention:**  Allows the model to attend to the **preceding tokens** in the sequence when predicting the next token. The \"masking\" prevents the model from \"seeing\" future tokens during training.\n",
    "            *   **Feed-Forward Network:** A two-layer fully connected network that applies a non-linear transformation to the output of the attention layer.\n",
    "    *   **Output Layer:**\n",
    "        *   A linear layer that projects the output of the final Transformer block to the size of the vocabulary.\n",
    "        *   A **softmax function** is applied to produce a probability distribution over the vocabulary, where each element represents the probability of the corresponding token being the next word in the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Figure 7. GPT-2 Architecture**\n",
    "\n",
    "![Figure 7](images/Figure%207.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Figure 8. Overview of GPT-2 Model Code**\n",
    "\n",
    "![Figure 7-1](images/Figure%207-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Key Concepts:**\n",
    "\n",
    "*   GPT-2 is a **decoder-only Transformer** with multiple stacked layers.\n",
    "*   **Token embeddings** represent words as vectors.\n",
    "*   **Positional encodings** provide information about token order.\n",
    "*   **Transformer blocks** use **masked self-attention** and **feed-forward networks** to process the sequence.\n",
    "*   The **output layer** produces a probability distribution over the vocabulary.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQRw7J39OQOR",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **3.2 Implementing the GPT-2 Model from Scratch**\n",
    "\n",
    "*   **Skeleton Code:**\n",
    "\n",
    " We'll start with a basic framework for the GPT-2 class in PyTorch. The goal of this code is to show you how the different parts of a Transformer model work by implementing them from scratch. Don't worry if you don't understand all of it right away! We will go through it step by step.\n",
    "\n",
    "```\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward, dropout=0.1, layer_norm_epsilon=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Token Embedding Layer\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # 2. Positional Encoding Layer (using fixed sinusoidal encodings)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        # 3. Transformer Blocks (Decoder Layers)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, nhead, dim_feedforward, dropout, layer_norm_epsilon) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # 4. Output Layer\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # Weight tying (optional but often beneficial)\n",
    "        self.output_layer.weight = self.token_embedding.weight\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # 1. Get token embeddings\n",
    "        x = self.token_embedding(input_ids) * math.sqrt(self.d_model) # Scale embeddings\n",
    "\n",
    "        # 2. Add positional encodings\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # 3. Create causal attention mask (to prevent attending to future tokens)\n",
    "        if attention_mask is None:\n",
    "            seq_len = input_ids.size(1)\n",
    "            attention_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "        # Invert the mask for compatibility with PyTorch's masking convention\n",
    "        attention_mask = attention_mask.logical_not()\n",
    "\n",
    "        # 4. Pass through Transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "          x = block(x, attention_mask)\n",
    "\n",
    "        # 5. Calculate output logits\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        return logits\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, d_model, nhead, dim_feedforward, dropout, layer_norm_epsilon):\n",
    "            super().__init__()\n",
    "\n",
    "            # Multi-head attention with efficient implementations\n",
    "            # Shape (batch, seq_len, d_model), nhead = heads, parallel attention\n",
    "            self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "\n",
    "            # Feed-forward network\n",
    "            self.ffn = nn.Sequential(\n",
    "                nn.Linear(d_model, dim_feedforward), # 768 -> 3072\n",
    "                nn.GELU(), # Non-linear\n",
    "                nn.Linear(dim_feedforward, d_model), #3078 -> 768\n",
    "                nn.Dropout(dropout) #Overfitting\n",
    "            )\n",
    "\n",
    "            # Layer normalization tto stablize\n",
    "            self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_epsilon)\n",
    "            self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_epsilon)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            \n",
    "        #    \n",
    "\n",
    "        def forward(self, x, attention_mask):\n",
    "            # Multi-head attention + residual connection\n",
    "            attn_output, _ = self.self_attn(x, x, x, attn_mask=attention_mask, need_weights=False)\n",
    "            x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "            # Feed-forward network + residual connection\n",
    "            ffn_output = self.ffn(x)\n",
    "            x = self.norm2(x + self.dropout(ffn_output))\n",
    "\n",
    "            return x\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```\n",
    "    class PositionalEncoding(nn.Module):\n",
    "        def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "            position = torch.arange(max_len).unsqueeze(1)\n",
    "            div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "            pe = torch.zeros(max_len, 1, d_model)\n",
    "            pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "            pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "            self.register_buffer('pe', pe)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x + self.pe[:x.size(1)].transpose(0, 1)\n",
    "            return self.dropout(x)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*   **Step-by-Step Explanation:**\n",
    "\n",
    "    *   **Token Embeddings:**\n",
    "         *   The `nn.Embedding(vocab_size, d_model)` creates the embedding layer, a lookup table where each row is a vector that represents a token.\n",
    "         *   `vocab_size` is the number of unique tokens in your vocabulary.\n",
    "         *   `d_model` is the dimensionality of the embedding vectors (e.g., 768 in GPT-2 small), this determines how detailed the information the vector can store about the token.\n",
    "         *  In the `forward` method, `input_ids` (a tensor of token IDs) is passed to `self.token_embedding` to get the corresponding embeddings. This means we are looking up the vector associated with each token id.\n",
    "\n",
    "     *   **Positional Encodings (Fixed Sinusoidal):**\n",
    "        *   Unlike Recurrent Neural Networks which processes the input sequentially, the Transformer treats all words as being equally far away from each other. The `PositionalEncoding` class adds information about the **position** of words in the sequence to the embeddings using sin and cos functions. This allows the Transformer to understand the order of words in the sequence.\n",
    "        *   It is added to the token embeddings in the `forward` method.\n",
    "\n",
    "    *   **Transformer Block:**\n",
    "         *   **Multi-Head Attention:**\n",
    "            *   `nn.MultiheadAttention(d_model, nhead, dropout=dropout)` creates the multi-head self-attention layer.\n",
    "            *   `d_model` is the dimensionality of the input and output (same as the embedding dimension).\n",
    "            *   `nhead` is the number of attention heads (e.g., 12 in GPT-2 small). Having multiple heads allows the model to attend to different types of relations between the words.\n",
    "            *   `dropout` is the dropout rate, a technique to prevent overfitting.\n",
    "            *   In the `forward` method:\n",
    "                *   `x` is passed as the query, key, and value to `self.self_attn`.\n",
    "                *   `attention_mask` is used for masking (we prevent the model from attending to future tokens during training).\n",
    "                *   The output of the attention layer is added to the original input (`x`) - this is called a **residual connection** which makes training deeper networks possible.\n",
    "                *   Layer normalization (`self.norm1`) is applied.\n",
    "         *   **Feed-Forward Network:**\n",
    "            *    `nn.Sequential` creates a simple feed-forward network with two linear layers and an activation function (GELU).\n",
    "            *   `dim_feedforward` is the dimensionality of the hidden layer in the feed-forward network (e.g., 3072 in GPT-2 small), this determines the complexity of relations learned between words.\n",
    "            *   In the `forward` method:\n",
    "                *   The output of the attention layer is passed through the feed-forward network.\n",
    "                *   Another residual connection is added.\n",
    "                *   Layer normalization (`self.norm2`) is applied.\n",
    "        *   **Layer Normalization**:\n",
    "            *  Layer normalization is a technique that helps stabilize training and improve model performance. It normalizes the activations across the feature dimension.\n",
    "\n",
    "       *  **Residual Connections**:\n",
    "            *  Residual connections help to prevent the vanishing gradient problem and allow for the training of deeper networks. They add the original input back to the output of a layer, making it easier for gradients to flow during backpropagation.\n",
    "    *   **Output Layer & Logits Calculation:**\n",
    "        *   `nn.Linear(d_model, vocab_size)` creates the output layer, which is a linear projection to the size of the vocabulary.\n",
    "        *   In the `forward` method, the output of the final Transformer block (`x`) is passed through the output layer to produce the `logits`.\n",
    "        *   The `logits` represent the unnormalized scores for each token in the vocabulary. A softmax function will be applied later (during training or inference) to convert these scores into probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*   **Modifications in GPT-2 Compared to Original Transformer:**\n",
    "     *   **Layer Normalization Placement:** GPT-2 applies layer normalization **before** the self-attention and feed-forward layers within each block, and also adds an additional layer normalization after the final Transformer block. The original Transformer applied it after these sub-layers.\n",
    "    *   **Activation Function:** GPT-2 uses the **GeLU (Gaussian Error Linear Unit)** activation function in the feed-forward network, while the original Transformer used ReLU. GeLU is considered to be smoother and can lead to better performance in some cases.\n",
    "    *   **Weight Tying:** In this example, the **weights of the token embedding layer are tied to the weights of the output layer**. This means that the same matrix is used for both embedding and projecting back to the vocabulary space. This can reduce the number of parameters and improve generalization.\n",
    "    *  **Initialization:** GPT-2 uses a modified initialization scheme where the weights of residual layers are scaled by a factor of  $\\\\frac{1}{\\\\sqrt{N}}$, where N is the number of layers. This is not done in the provided code for simplicity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Key Concepts:**\n",
    "*   The code implements the key components of the GPT-2 architecture: **token embeddings, Transformer blocks (with masked self-attention and feed-forward networks), and the output layer**.\n",
    "*   **Layer normalization** and **residual connections** are important for training stability and performance.\n",
    "*   GPT-2 has some modifications compared to the original Transformer, such as the **placement of layer normalization** and the use of the **GeLU activation**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeWrXgG1OqcT",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **3.3 Hands-on Exercise 2: Implementing the GPT-2 Model**\n",
    "\n",
    "*   **Activity:** Students will understand how the code works and implement the Transformer model.\n",
    "*   **Task:**\n",
    "    1. Understand the code for `TransformerBlock` and `GPT2`.\n",
    "    2.  Explain what each line of code does.\n",
    "*   **Guidance:**\n",
    "    *   Refer to the explanations and code snippets in section 3.2.\n",
    "    *   Use the PyTorch documentation for `nn.MultiheadAttention`, `nn.Sequential`, `nn.Linear`, `nn.LayerNorm`, and other relevant modules.\n",
    "    *   Pay attention to the order of operations (attention, residual connection, layer normalization, feed-forward network, etc.).\n",
    "    *   Make sure the dimensions of the tensors are correct at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alIfOWRIPMdR",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Basic code for GPT-2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model,\n",
    "        nhead,\n",
    "        num_layers,\n",
    "        dim_feedforward,\n",
    "        dropout=0.1,\n",
    "        layer_norm_epsilon=1e-5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Token Embedding Layer\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # 2. Positional Encoding Layer (using fixed sinusoidal encodings)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        # 3. Transformer Blocks (Decoder Layers)\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    d_model, nhead, dim_feedforward, dropout, layer_norm_epsilon\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 4. Output Layer\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # Weight tying (optional but often beneficial)\n",
    "        self.output_layer.weight = self.token_embedding.weight\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # 1. Get token embeddings\n",
    "        x = self.token_embedding(input_ids) * math.sqrt(\n",
    "            self.d_model\n",
    "        )  # Scale embeddings\n",
    "\n",
    "        # 2. Add positional encodings\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # 3. Create causal attention mask (to prevent attending to future tokens)\n",
    "        if attention_mask is None:\n",
    "            seq_len = input_ids.size(1)\n",
    "            attention_mask = torch.triu(\n",
    "                torch.ones(seq_len, seq_len, device=x.device, dtype=torch.bool),\n",
    "                diagonal=1,\n",
    "            )\n",
    "\n",
    "        # Invert the mask for compatibility with PyTorch's masking convention\n",
    "        attention_mask = attention_mask.logical_not()\n",
    "\n",
    "        # 4. Pass through Transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, attention_mask)\n",
    "\n",
    "        # 5. Calculate output logits\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout, layer_norm_epsilon):\n",
    "        super().__init__()\n",
    "\n",
    "        # Multi-head attention with efficient implementations\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            d_model, nhead, dropout=dropout, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_epsilon)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_epsilon)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        # Multi-head attention + residual connection\n",
    "        attn_output, _ = self.self_attn(\n",
    "            x, x, x, attn_mask=attention_mask, need_weights=False\n",
    "        )\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feed-forward network + residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(1)].transpose(0, 1)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "vocab_size = 50257  # GPT-2 vocabulary size\n",
    "d_model = 768  # Hidden size (embedding dimension)\n",
    "nhead = 12  # Number of attention heads\n",
    "num_layers = 12  # Number of transformer layers\n",
    "dim_feedforward = 3072  # Feedforward network dimension\n",
    "batch_size = 8\n",
    "seq_length = 1024\n",
    "\n",
    "model = GPT2(vocab_size, d_model, nhead, num_layers, dim_feedforward)\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "\n",
    "# Generate output\n",
    "output = model(input_ids)\n",
    "print(output.shape)  # Should be (batch_size, seq_length, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LS29K1CXXG4S",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.4 Hands-on Exercise 3: Pre-Training GPT-2\n",
    "\n",
    "\n",
    "*   **Concept and Importance:**\n",
    "\n",
    "    *   **Pre-training** is the process of training a language model on a **massive dataset of general text** to learn the fundamental patterns and structures of language.\n",
    "    *   It's like giving the model a broad education in language before teaching it more specific tasks.\n",
    "    *   **Why is it important?**\n",
    "        *   **Learns General Language Understanding:** The model acquires a rich internal representation of language, including syntax, semantics, and even some world knowledge.\n",
    "        *   **Data Efficiency for Downstream Tasks:** Pre-trained models can be fine-tuned for specific tasks with much less data than training from scratch.\n",
    "        *   **Improved Performance:** Fine-tuned models often achieve better performance on downstream tasks compared to models trained from scratch.\n",
    "    *   **Analogy:** Think of pre-training as learning the basic rules of grammar and vocabulary, and fine-tuning as learning how to write a specific type of essay or answer a specific type of question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*   **Training Objective:**\n",
    "\n",
    "    *   The primary training objective in pre-training GPT-2 is **next-word prediction** (also called **language modeling** or **causal language modeling**).\n",
    "    *   The model is given a sequence of words and is trained to predict the probability distribution of the next word in the sequence.\n",
    "    *   **Example:**\n",
    "        *   Input: \"The quick brown fox\"\n",
    "        *   Target: \"jumps\"\n",
    "        *   The model is trained to assign a high probability to \"jumps\" given the preceding words.\n",
    "\n",
    "*   **Loss Function and Optimization:**\n",
    "\n",
    "    *   **Loss Function:** The standard loss function used is **cross-entropy loss**. It measures the difference between the model's predicted probability distribution over the vocabulary and the actual distribution (where the target word has a probability of 1 and all other words have a probability of 0).\n",
    "    *   **Optimization:** An optimizer (like AdamW) is used to update the model's parameters to minimize the loss function. The gradients are calculated using backpropagation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Key Concepts:**\n",
    "\n",
    "*   **Pre-training** is training on a large general text dataset to learn broad language understanding.\n",
    "*   The **training objective** is next-word prediction.\n",
    "*   **Cross-entropy loss** is used to measure the difference between the predicted and actual word distributions.\n",
    "*   An **optimizer** updates the model's parameters to minimize the loss.\n",
    "\n",
    "**Activity:** Students will run the training code we developed. We will use a very small dataset for demonstration purposes.\n",
    "*   **Task:**\n",
    "    1.  Run the provided training code.\n",
    "    2.  Observe the loss curve during training.\n",
    "    3.  Experiment with different hyperparameters (optional).\n",
    "    4.  Try out text completion prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBkXiipJXS3j",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Pre-Training GPT-2. NOTE MODIFIED TO RUN ON COLAB i.e. small-GPT\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from tqdm import tqdm  # For a nice progress bar\n",
    "import tiktoken\n",
    "\n",
    "#class GPT2(nn.Module):\n",
    "    # ... (Refer to previous code)\n",
    "\n",
    "#class TransformerBlock(nn.Module):\n",
    "    # ... (Refer to previous code)\n",
    "\n",
    "#class PositionalEncoding(nn.Module):\n",
    "    # ... (Refer to previous code)\n",
    "\n",
    "# --- Data Preparation (using a dummy dataset for demonstration) ---\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_data, sequence_length, tokenizer=None):\n",
    "        self.sequence_length = sequence_length\n",
    "        if tokenizer is None:\n",
    "            self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        \n",
    "        # Tokenize the entire text at once\n",
    "        self.tokens = self.tokenizer.encode(text_data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.tokens) - self.sequence_length - 1)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sequence and ensure it's exactly sequence_length\n",
    "        input_sequence = self.tokens[idx:idx + self.sequence_length]\n",
    "        target_sequence = self.tokens[idx + 1:idx + self.sequence_length + 1]\n",
    "        \n",
    "        # Pad if necessary (shouldn't be needed if data is long enough)\n",
    "        if len(input_sequence) < self.sequence_length:\n",
    "            padding = [0] * (self.sequence_length - len(input_sequence))\n",
    "            input_sequence = input_sequence + padding\n",
    "            target_sequence = target_sequence + padding\n",
    "            \n",
    "        # Truncate if somehow longer\n",
    "        input_sequence = input_sequence[:self.sequence_length]\n",
    "        target_sequence = target_sequence[:self.sequence_length]\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(input_sequence, dtype=torch.long),\n",
    "            torch.tensor(target_sequence, dtype=torch.long)\n",
    "        )\n",
    "    \n",
    "# --- Hyperparameters and Configuration ---\n",
    "\n",
    "vocab_size = 50257  # GPT-2 vocabulary size\n",
    "d_model = 128  # Hidden size (embedding dimension)\n",
    "nhead = 4  # Number of attention heads\n",
    "num_layers = 2 # Number of transformer layers\n",
    "dim_feedforward = 128  # Feedforward network dimension\n",
    "dropout = 0.1\n",
    "batch_size = 4\n",
    "sequence_length = 128\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 2 # You can increase this for better results if you have the resources\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Create the Model, Optimizer, and Loss Function ---\n",
    "\n",
    "model = GPT2(vocab_size, d_model, nhead, num_layers, dim_feedforward, dropout).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)  # Using AdamW optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Learning Rate Scheduler (example with linear decay) ---\n",
    "\n",
    "# We'll use a dummy dataset for this example. Replace with your actual dataset.\n",
    "text_data = \"This is a very long text used as a dummy example for training a GPT-2 model. You would replace this with a much larger dataset in a real-world scenario.\" * 50\n",
    "# If you have a real dataset, you can uncomment and modify the following lines:\n",
    "# with open(\"your_dataset.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     text_data = f.read()\n",
    "\n",
    "num_training_steps = num_epochs * (len(text_data) - sequence_length) // batch_size  # Approximate number of training steps\n",
    "lr_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_training_steps)\n",
    "\n",
    "dataset = TextDataset(text_data, sequence_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# --- Training Loop ---\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (input_ids, target_ids) in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs.view(-1, vocab_size), target_ids.view(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients (optional but often helpful for Transformers)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update learning rate\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}, LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "print(\"Training finished!\")\n",
    "\n",
    "# --- Save the trained model ---\n",
    "# This saves the trained model's state_dict, which contains the model's learned parameters.\n",
    "# You can load the state_dict later to resume training or use the model for inference.\n",
    "torch.save(model.state_dict(), \"trained_gpt2_model.pth\")\n",
    "print(\"Model saved to trained_gpt2_model.pth\")\n",
    "\n",
    "# --- Text Completion Demonstration ---\n",
    "# Now let's use the trained model to complete some text.\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "prompt = \"The quick brown fox\"\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients during inference\n",
    "    for _ in range(50):  # Generate 50 tokens\n",
    "        outputs = model(input_ids)\n",
    "        predicted_token_id = outputs[0, -1, :].argmax().item()  # Get the ID of the most likely next token\n",
    "        input_ids = torch.cat((input_ids, torch.tensor([[predicted_token_id]], dtype=torch.long).to(device)), dim=1)  # Append the predicted token to the input\n",
    "        if predicted_token_id == tokenizer.eot_token:\n",
    "            break\n",
    "\n",
    "generated_text = tokenizer.decode(input_ids[0].tolist())  # Decode the generated tokens back into text\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A More Modern Implementation MODIFIED TO RUN ON COLAB\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get the best available device\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():  # For M1/M2 Macs\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "def create_dataset():\n",
    "    # Create a small dataset for testing\n",
    "    text = [\n",
    "        \"GPT models learn to generate text.\",\n",
    "        \"Machine learning is fascinating.\",\n",
    "        \"Neural networks process data.\"\n",
    "    ] * 20\n",
    "    return Dataset.from_dict({\"text\": text})\n",
    "\n",
    "def train_tiny_gpt2(device=None):\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    \n",
    "    # Adjust model size based on device\n",
    "    if device == \"cpu\" or device == \"mps\":\n",
    "        # Smaller model for CPU/MPS\n",
    "        model_config = {\n",
    "            \"n_layer\": 2,     # 2 layers\n",
    "            \"n_head\": 4,      # 4 attention heads\n",
    "            \"n_embd\": 128,    # 128 dim embeddings\n",
    "        }\n",
    "    else:\n",
    "        # Slightly larger model for GPU\n",
    "        model_config = {\n",
    "            \"n_layer\": 4,     # 4 layers\n",
    "            \"n_head\": 8,      # 8 attention heads\n",
    "            \"n_embd\": 256,    # 256 dim embeddings\n",
    "        }\n",
    "    \n",
    "    model = GPT2LMHeadModel.from_pretrained(\n",
    "        \"gpt2\",\n",
    "        **model_config,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Set padding token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Create and tokenize dataset\n",
    "    dataset = create_dataset()\n",
    "    \n",
    "    def tokenize(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            max_length=32,\n",
    "            padding='max_length'\n",
    "        )\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize,\n",
    "        remove_columns=dataset.column_names,\n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    # Adjust batch size and learning rate based on device\n",
    "    if device == \"cpu\":\n",
    "        batch_size = 2\n",
    "        learning_rate = 5e-4\n",
    "    elif device == \"mps\":\n",
    "        batch_size = 4\n",
    "        learning_rate = 1e-3\n",
    "    else:\n",
    "        batch_size = 8\n",
    "        learning_rate = 2e-3\n",
    "    \n",
    "    # Training configuration\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./tiny-gpt2\",\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"none\",\n",
    "        # Memory optimizations\n",
    "        gradient_accumulation_steps=2,\n",
    "        fp16=(device == \"cuda\"),  # Use fp16 only on CUDA\n",
    "        dataloader_pin_memory=(device != \"cpu\"),\n",
    "        # Prevent OOM on different devices\n",
    "        max_grad_norm=0.5,\n",
    "        warmup_steps=100,\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Train with error handling\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {e}\")\n",
    "        # Try to reduce model size and retry\n",
    "        if device != \"cpu\":\n",
    "            print(\"Reducing model size and retrying...\")\n",
    "            return train_tiny_gpt2(\"cpu\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_text(model, tokenizer, input_text, device=None):\n",
    "    \"\"\"Generate text with memory-efficient settings\"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    \n",
    "    try:\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Memory-efficient generation settings\n",
    "        gen_kwargs = {\n",
    "            \"max_length\": 20,\n",
    "            \"num_beams\": 1,\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\": 0.9,\n",
    "            \"temperature\": 0.7,\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, **gen_kwargs)\n",
    "        \n",
    "        # Clear cache if using GPU\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Generation error: {e}\")\n",
    "        return f\"Error generating text: {str(e)}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Train model\n",
    "    device = get_device()\n",
    "    model, tokenizer = train_tiny_gpt2(device)\n",
    "    \n",
    "    # Test generation\n",
    "    input_text = \"Neural networks\"\n",
    "    output_text = generate_text(model, tokenizer, input_text, device)\n",
    "    print(f\"\\nInput: {input_text}\")\n",
    "    print(f\"Output: {output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XA3M05-lNcp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Part 4: Post-Processing: Fine-Tuning**\n",
    "\n",
    "**Learning Objectives:**\n",
    "* Understand the benefits of using pre-trained weights.\n",
    "* Be able to load pre-trained models and weights.\n",
    "* Understand fine-tuning and instruction fine-tuning\n",
    "* Know of existing Instruction Fine-tuning datasets.\n",
    "* Be able to fine-tune a pre-trained LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d7oO8ennPcs",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **4.1 Loading Pre-trained Weights**\n",
    "\n",
    "*   **Benefits:**\n",
    "\n",
    "    *   **Saves Time and Resources:** You don't have to pre-train the model from scratch, which can take days, weeks, or even months, and requires a lot of computational power.\n",
    "    *   **Better Performance:** Pre-trained weights have already learned a good representation of language, so starting from these weights usually leads to better fine-tuning results.\n",
    "    *   **Accessibility:** Organizations like OpenAI and Hugging Face have made pre-trained weights for various models (including GPT-2) publicly available.\n",
    "\n",
    "*   **Implementation (using Hugging Face `transformers`):**\n",
    "\n",
    "    *   The Hugging Face `transformers` library is the easiest and most common way to load pre-trained GPT-2 weights.\n",
    "\n",
    "*   **Explanation:**\n",
    "\n",
    "    *   `from_pretrained(model_name)` automatically downloads and loads the pre-trained weights and configuration for the specified model name (e.g., \"gpt2\", \"gpt2-medium\", etc.).\n",
    "    *   `GPT2LMHeadModel` is the Hugging Face class for GPT-2 used for language modeling (with a language modeling head on top).\n",
    "    *   `GPT2Tokenizer` is the corresponding tokenizer.\n",
    "\n",
    "*   **OPTIONAL Loading into our Custom GPT-2:**\n",
    "    *   You can also load the state dict into our own GPT-2 class.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VdfnIJJn8nw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Example inference with proper attention mask\n",
    "input_text = \"The quick brown fox\"\n",
    "inputs = tokenizer(input_text, \n",
    "                  return_tensors=\"pt\",\n",
    "                  padding=True,\n",
    "                  truncation=True,\n",
    "                  max_length=100)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_length=100,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "*   **Pre-trained weights** provide a huge shortcut for training.\n",
    "*   The **Hugging Face `transformers` library** is the standard way to access and use pre-trained models.\n",
    "*   You can load weights into **custom model architectures** if needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eiyqrr7joPk0",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming you have defined the GPT2 class as before\n",
    "\n",
    "# Load the pre-trained model from Hugging Face\n",
    "pretrained_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Create an instance of your custom GPT2 model\n",
    "model = GPT2(vocab_size, d_model, nhead, num_layers, dim_feedforward, dropout)\n",
    "\n",
    "# Transfer the weights from the pre-trained model to your model\n",
    "# This requires careful mapping of the layer names\n",
    "model_state_dict = model.state_dict()\n",
    "pretrained_state_dict = {}\n",
    "for name, param in pretrained_model.named_parameters():\n",
    "    # Map the parameter names from the pre-trained model to your model's state_dict\n",
    "    if \"transformer.h\" in name:\n",
    "        name = name.replace(\"transformer.h\", \"transformer_blocks\")\n",
    "    elif \"transformer.wte\" in name:\n",
    "        name = name.replace(\"transformer.wte\", \"token_embedding\")\n",
    "    elif \"transformer.wpe\" in name:\n",
    "        name = name.replace(\"transformer.wpe\", \"positional_encoding\")\n",
    "    elif \"transformer.ln_f\" in name:\n",
    "        name = name.replace(\"transformer.ln_f\", \"final_layer_norm\")\n",
    "    elif \"lm_head\" in name:\n",
    "        name = name.replace(\"lm_head\", \"output_layer\")\n",
    "\n",
    "    # Ensure the parameter exists in your model's state_dict\n",
    "    if name in model_state_dict:\n",
    "        pretrained_state_dict[name] = param\n",
    "\n",
    "# Load the pre-trained weights into your model\n",
    "model.load_state_dict(pretrained_state_dict, strict=False)\n",
    "\n",
    "# Verify the parameters have been loaded correctly\n",
    "for name, param in model.named_parameters():\n",
    "    if name in pretrained_state_dict:\n",
    "        assert torch.allclose(\n",
    "            param, pretrained_state_dict[name]\n",
    "        ), f\"Mismatch in parameter {name}\"\n",
    "\n",
    "# You can now use your custom GPT-2 model with the loaded pre-trained weights\n",
    "print(\"Pre-trained weights loaded into custom GPT-2 model successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dj-jtdxdlWAt",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **4.2 Fine-tuning for Specific Tasks**\n",
    "\n",
    "*   **Concept of Fine-tuning:**\n",
    "\n",
    "    *   **Fine-tuning** is the process of taking a **pre-trained model** and **further training it on a smaller dataset** that is specific to a particular downstream task.\n",
    "    *   The pre-trained weights are used as a starting point, and the model's parameters are adjusted to better perform the new task.\n",
    "    *   **Example Tasks:**\n",
    "        *   **Text Classification:** Classifying text into different categories (e.g., sentiment analysis, topic classification).\n",
    "        *   **Question Answering:** Answering questions based on a given context passage.\n",
    "        *   **Text Summarization:** Generating summaries of longer text documents.\n",
    "        *   **Translation:** (Though less common with decoder-only models like GPT-2, you could potentially fine-tune for a specific language pair).\n",
    "        *   **Instruction Following:**  Training the model to follow specific instructions (more on this below).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*   **Process:**\n",
    "\n",
    "    1.  **Load Pre-trained Weights:** Start with a pre-trained GPT-2 model (as described in section 4.3).\n",
    "    2.  **Prepare Task-Specific Data:** Create a dataset for your specific task. This dataset will usually be much smaller than the pre-training dataset.\n",
    "    3.  **Add a Task-Specific Output Layer (Optional):**\n",
    "        *   For some tasks (like classification), you might need to add a new output layer on top of the pre-trained model. For example, for binary classification, you could add a linear layer that outputs a single number, followed by a sigmoid activation function to produce a probability between 0 and 1.\n",
    "        *   For text generation tasks, the existing language modeling head (which produces a probability distribution over the vocabulary) might be sufficient.\n",
    "    4.  **Modify Training Objective (if needed):**\n",
    "        *   For example, if you add a classification head, you would use a classification loss (like binary cross-entropy).\n",
    "        *   For text generation\n",
    "\n",
    "    5.  **Train the Model:**\n",
    "        *   Train the model on your task-specific dataset, updating the weights of the entire model (or sometimes just the new output layer and a few of the top layers of the pre-trained model).\n",
    "        *   Use a lower learning rate than in pre-training to avoid drastic changes to the already learned representations.\n",
    "        *   Train for fewer epochs than in pre-training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*   **Instruction Fine-tuning**\n",
    "\n",
    "    *   This is a popular and effective way to fine-tune models to perform a wide range of tasks, by training it to follow instructions.\n",
    "    *   **Idea:** You provide the model with an **instruction** that describes the task you want it to perform, along with the input, and train it to generate the correct output that follows the instruction.\n",
    "    *   **Example:**\n",
    "\n",
    "        ```\n",
    "        Instruction: Translate the following English sentence to French.\n",
    "        Input: The cat sat on the mat.\n",
    "        Output: Le chat s'est assis sur le tapis.\n",
    "\n",
    "        Instruction: Summarize the following article.\n",
    "        Input: <Article text>\n",
    "        Output: <Summary of the article>\n",
    "\n",
    "        Instruction: Answer the following question based on the context provided.\n",
    "        Context: <Context text>\n",
    "        Question: What is the capital of France?\n",
    "        Output: Paris\n",
    "        ```\n",
    "\n",
    "    *   **Benefits:**\n",
    "        *   **Flexibility:** You can train a single model to perform many different tasks by simply changing the instruction.\n",
    "        *   **Generalization:** Models trained with instructions often generalize better to new, unseen instructions.\n",
    "        *   **Data Efficiency:** Can be more data-efficient than training separate models for each task.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*   **Dataset Format:** Instruction fine-tuning datasets typically consist of (instruction, input, output) triplets. You can create these datasets manually, or you can use existing datasets (see the next section).\n",
    "\n",
    "*   **Introducing some existing IFT datasets**\n",
    "\n",
    "    *   There are many publicly available instruction fine-tuning datasets that you can use to train your models. Here are a few examples:\n",
    "        *   **Dolly (Databricks):**\n",
    "            *   Created by Databricks, Dolly 15k contains 15,000 high-quality, human-generated prompt/response pairs specifically designed for instruction tuning large language models.\n",
    "            *   [https://huggingface.co/datasets/databricks/databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)\n",
    "        *   **OpenAssistant Conversations Dataset (OASST1):**\n",
    "            *   A human-generated, human-annotated assistant-style conversation corpus. It consists of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings.\n",
    "            *   [https://huggingface.co/datasets/OpenAssistant/oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1)\n",
    "        *   **Airoboros (jondurbin):**\n",
    "            *   This is a synthetic dataset generated by GPT-4 and fine-tuned on a LLaMA model. It's been shown to be quite effective for training instruction-following models.\n",
    "            *   [https://huggingface.co/datasets/jondurbin/airoboros-gpt4-m2.0](https://huggingface.co/datasets/jondurbin/airoboros-gpt4-m2.0)\n",
    "        *   **Many more**: There are many other instruction datasets available on platforms like Hugging Face Datasets. Look for datasets with keywords like \"instruction\", \"tuning\", \"assistant\", etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Key Concepts:**\n",
    "\n",
    "*   **Fine-tuning** adapts a pre-trained model to a specific task.\n",
    "*   **Instruction fine-tuning** trains a model to follow natural language instructions.\n",
    "*   There are many **publicly available instruction datasets** for various tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3OZ1DE3pG0a",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **4.3 Hands-on Exercise 4: Post-training Work**\n",
    "\n",
    "*   **Activity:** We'll fine-tune a pre-trained GPT-2 model for a simple instruction-following task. We will be using the Hugging Face `transformers` library to simplify the process.\n",
    "\n",
    "*   **Task:** Fine-tune GPT-2 to convert sentences to passive voice based on an instruction.\n",
    "\n",
    "*   **Dataset:** We'll create a very small, illustrative dataset for this example. In a real-world scenario, you would use a much larger dataset.\n",
    "\n",
    "*   **Steps:**\n",
    "    1.  **Load Pre-trained Model and Tokenizer:** We load the pre-trained GPT-2 model and tokenizer from Hugging Face `transformers`.\n",
    "    2.  **Prepare the Dataset:** We create a simple dataset with a few examples of active-to-passive voice conversion instructions.\n",
    "    3.  **Tokenize the Data:** We format our instruction, input and output text into a single string, and use `TextDataset` with `DataCollatorForLanguageModeling` to prepare the data for the `Trainer`.\n",
    "    4.  **Training Arguments:** We set up the training arguments, including the output directory, number of epochs, batch size, etc.\n",
    "    5.  **Trainer:** We use the Hugging Face `Trainer` class, which simplifies the training process. We pass the model, training arguments, data collator, and training dataset to the `Trainer`.\n",
    "    6.  **Fine-tune:** `trainer.train()` starts the fine-tuning process.\n",
    "    7.  **Save:** We save the fine-tuned model using `trainer.save_model()`.\n",
    "    8.  **Inference:** We test the model by giving it a new instruction and input, and generating the output.\n",
    "\n",
    "*   **Guidance:**\n",
    "    *   Run the code and observe the output.\n",
    "    *   Experiment with different instructions and inputs.\n",
    "    *   Try adding more examples to the `instruction_data` to improve the model's performance.\n",
    "    *   You can adjust the `max_length` parameter in `model.generate()` to control the length of the generated text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Simple instruction fine-tuning\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Check if MPS is available\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create a custom dataset class\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = item['input_ids'].clone()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move model to MPS device\n",
    "model = model.to(device)\n",
    "\n",
    "# Prepare the dataset\n",
    "instruction_data = [\n",
    "    {\n",
    "        \"instruction\": \"Convert the following sentence to passive voice.\",\n",
    "        \"input\": \"The cat chased the mouse.\",\n",
    "        \"output\": \"The mouse was chased by the cat.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Convert the following sentence to passive voice.\",\n",
    "        \"input\": \"John ate the apple.\",\n",
    "        \"output\": \"The apple was eaten by John.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Convert the following sentence to passive voice.\",\n",
    "        \"input\": \"The teacher graded the tests.\",\n",
    "        \"output\": \"The tests were graded by the teacher.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Convert the following sentence to passive voice.\",\n",
    "        \"input\": \"The storm damaged the house.\",\n",
    "        \"output\": \"The house was damaged by the storm.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Convert the following sentence to passive voice.\",\n",
    "        \"input\": \"The company hired new employees.\",\n",
    "        \"output\": \"New employees were hired by the company.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Convert the following sentence to passive voice.\",\n",
    "        \"input\": \"Students submitted their assignments.\",\n",
    "        \"output\": \"The assignments were submitted by the students.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Format the texts\n",
    "train_texts = [\n",
    "    f\"### Instruction: {data['instruction']}\\n### Input: {data['input']}\\n### Output: {data['output']}\\n### End\"\n",
    "    for data in instruction_data\n",
    "]\n",
    "\n",
    "# Create dataset using custom class\n",
    "train_dataset = InstructionDataset(\n",
    "    texts=train_texts,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_passive_voice\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "test_instruction = \"Convert the following sentence to passive voice.\"\n",
    "test_input = \"The chef prepared the meal.\"\n",
    "prompt = f\"Instruction: {test_instruction}\\nInput: {test_input}\\nOutput:\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# Move inputs to MPS device\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=100,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"\\nGenerated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modern implementation\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def setup_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Set up the appropriate device for training.\n",
    "    Supports: CUDA (Colab/GPU), MPS (Apple Silicon), CPU\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using Apple Silicon MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "def is_colab() -> bool:\n",
    "    \"\"\"Check if we're running in Google Colab.\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def prepare_instruction_data() -> list:\n",
    "    \"\"\"\n",
    "    Prepare the instruction dataset with more examples for better training.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        # Passive Voice Examples\n",
    "        {\n",
    "            \"instruction\": \"Convert the following sentence to passive voice.\",\n",
    "            \"input\": \"The cat chased the mouse.\",\n",
    "            \"output\": \"The mouse was chased by the cat.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Convert the following sentence to passive voice.\",\n",
    "            \"input\": \"John ate the apple.\",\n",
    "            \"output\": \"The apple was eaten by John.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Convert the following sentence to passive voice.\",\n",
    "            \"input\": \"The teacher graded the tests.\",\n",
    "            \"output\": \"The tests were graded by the teacher.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Convert the following sentence to passive voice.\",\n",
    "            \"input\": \"The storm damaged the house.\",\n",
    "            \"output\": \"The house was damaged by the storm.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Convert the following sentence to passive voice.\",\n",
    "            \"input\": \"The students completed the project.\",\n",
    "            \"output\": \"The project was completed by the students.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Convert the following sentence to passive voice.\",\n",
    "            \"input\": \"The chef prepared the meal.\",\n",
    "            \"output\": \"The meal was prepared by the chef.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Convert the following sentence to passive voice.\",\n",
    "            \"input\": \"The company launched a new product.\",\n",
    "            \"output\": \"A new product was launched by the company.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Convert the following sentence to passive voice.\",\n",
    "            \"input\": \"The artist painted the portrait.\",\n",
    "            \"output\": \"The portrait was painted by the artist.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Convert the following sentence to passive voice.\",\n",
    "            \"input\": \"The team won the championship.\",\n",
    "            \"output\": \"The championship was won by the team.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Convert the following sentence to passive voice.\",\n",
    "            \"input\": \"The researcher published the findings.\",\n",
    "            \"output\": \"The findings were published by the researcher.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "def format_instruction(example: dict) -> str:\n",
    "    \"\"\"Format a single instruction example.\"\"\"\n",
    "    return f\"\"\"### Instruction: {example['instruction']}\n",
    "### Input: {example['input']}\n",
    "### Output: {example['output']}\n",
    "### End\"\"\"\n",
    "\n",
    "def create_train_eval_datasets(instruction_data: list, tokenizer: AutoTokenizer) -> tuple[Dataset, Dataset]:\n",
    "    \"\"\"Create training and evaluation datasets.\"\"\"\n",
    "    # Split data into train and eval (80/20 split)\n",
    "    train_size = int(0.8 * len(instruction_data))\n",
    "    train_data = instruction_data[:train_size]\n",
    "    eval_data = instruction_data[train_size:]\n",
    "    \n",
    "    def create_dataset(data):\n",
    "        texts = [format_instruction(example) for example in data]\n",
    "        dataset_dict = {\"text\": texts}\n",
    "        dataset = Dataset.from_dict(dataset_dict)\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(\n",
    "                examples[\"text\"],\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=128,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        \n",
    "        return dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "    \n",
    "    return create_dataset(train_data), create_dataset(eval_data)\n",
    "\n",
    "def setup_model_and_tokenizer(model_name: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    Set up the model and tokenizer with platform-specific optimizations.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Configure model based on platform\n",
    "    if device.type == \"cuda\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "    elif device.type == \"mps\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "    \n",
    "    # Add padding token if it doesn't exist\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    model = model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "def setup_training_arguments(output_dir: str, device: torch.device) -> TrainingArguments:\n",
    "    \"\"\"Set up training arguments optimized for the current platform.\"\"\"\n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=2 if device.type in [\"cuda\", \"mps\"] else 1,\n",
    "        per_device_eval_batch_size=2 if device.type in [\"cuda\", \"mps\"] else 1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=50,\n",
    "        logging_steps=10,\n",
    "        save_steps=100,\n",
    "        eval_steps=100,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"tensorboard\",\n",
    "        fp16=device.type == \"cuda\",\n",
    "        # Add metric for evaluation\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False\n",
    "    )\n",
    "\n",
    "def generate_test_output(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    test_input: str,\n",
    "    device: torch.device\n",
    ") -> str:\n",
    "    \"\"\"Generate output for a test input.\"\"\"\n",
    "    prompt = f\"### Instruction: Convert the following sentence to passive voice.\\n### Input: {test_input}\\n### Output:\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=100,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def test_model(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, device: torch.device):\n",
    "    \"\"\"Test the model with multiple examples.\"\"\"\n",
    "    test_inputs = [\n",
    "        \"The programmer wrote the code.\",\n",
    "        \"The audience applauded the performance.\",\n",
    "        \"The sun melted the snow.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting the model with multiple examples:\")\n",
    "    for test_input in test_inputs:\n",
    "        generated_text = generate_test_output(model, tokenizer, test_input, device)\n",
    "        print(f\"\\nInput: {test_input}\")\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "\n",
    "def main():\n",
    "    # Setup output directory based on environment\n",
    "    if is_colab():\n",
    "        # Uncomment for Google Drive integration\n",
    "        # from google.colab import drive\n",
    "        # drive.mount('/content/drive')\n",
    "        # output_dir = \"/content/drive/MyDrive/passive_voice_model\"\n",
    "        output_dir = \"./passive_voice_model\"\n",
    "    else:\n",
    "        output_dir = \"./passive_voice_model\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Setup device\n",
    "    device = setup_device()\n",
    "    \n",
    "    # Use distilgpt2 for faster training, or gpt2 for better results\n",
    "    model_name = \"distilgpt2\"\n",
    "    logger.info(f\"Using model: {model_name}\")\n",
    "    \n",
    "    # Prepare model and tokenizer\n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    \n",
    "    # Prepare data\n",
    "    instruction_data = prepare_instruction_data()\n",
    "    train_dataset, eval_dataset = create_train_eval_datasets(instruction_data, tokenizer)\n",
    "    \n",
    "    logger.info(f\"Training examples: {len(train_dataset)}\")\n",
    "    logger.info(f\"Evaluation examples: {len(eval_dataset)}\")\n",
    "    \n",
    "    # Setup training arguments\n",
    "    training_args = setup_training_arguments(output_dir, device)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    logger.info(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save final model\n",
    "    logger.info(\"Saving model...\")\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # Test the model\n",
    "    logger.info(\"Testing model...\")\n",
    "    test_model(model, tokenizer, device)\n",
    "    \n",
    "    # Print instructions for tensorboard\n",
    "    print(\"\\nTo view training metrics:\")\n",
    "    if is_colab():\n",
    "        print(\"Run these commands:\")\n",
    "        print(\"%load_ext tensorboard\")\n",
    "        print(f\"%tensorboard --logdir {output_dir}/runs\")\n",
    "    else:\n",
    "        print(\"Run this command in terminal:\")\n",
    "        print(f\"tensorboard --logdir {output_dir}/runs\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./passive_voice_model/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*   **Optional:**\n",
    "    *   Explore other fine-tuning tasks like text summarization or question answering. You will need to find suitable datasets for these tasks. For example, you can load a summarization dataset from Hugging Face `datasets` like this:\n",
    "\n",
    "        ```python\n",
    "        from datasets import load_dataset\n",
    "        dataset = load_dataset(\"xsum\")  # Load the XSum summarization dataset\n",
    "        ```\n",
    "\n",
    "        Then you'll need to preprocess the dataset to fit the input format of your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "*   Students should be able to fine-tune a pre-trained GPT-2 model for a simple instruction-following task using the Hugging Face `transformers` library.\n",
    "*   They should understand the basic process of preparing data for fine-tuning.\n",
    "*   They should be able to test the fine-tuned model with new inputs.\n",
    "*   They should be aware that this is a simplified example and that real-world fine-tuning often involves larger datasets, more complex data processing, and more careful evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FY58HzsyzXki",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Part 5: Conclusion and Next Steps**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8XnwfEHzToL",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **5.1 Recap of Key Concepts**\n",
    "\n",
    "*   **Large Language Models (LLMs):**\n",
    "    *   Powerful deep learning models trained on massive text data.\n",
    "    *   Capable of understanding and generating human-like text.\n",
    "    *   Examples: GPT-2, GPT-3, BERT, LaMDA, etc.\n",
    "*   **GPT-2:**\n",
    "    *   A decoder-only Transformer model.\n",
    "    *   Designed for text generation (next-word prediction).\n",
    "    *   Good for learning the fundamentals of LLMs due to its relatively simpler architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*   **Transformers:**\n",
    "    *   Revolutionized NLP with the \"Attention is All You Need\" paper.\n",
    "    *   Key components:\n",
    "        *   **Self-Attention:** Allows the model to weigh the importance of different words in a sequence.\n",
    "        *   **Multi-Head Attention:** Multiple attention mechanisms working in parallel.\n",
    "        *   **Positional Encoding:** Provides information about word order.\n",
    "        *   **Feed-Forward Networks:** Apply non-linear transformations.\n",
    "        *   **Layer Normalization:** Stabilizes training.\n",
    "        *   **Residual Connections:** Help with gradient flow.\n",
    "    *   **Advantages over RNNs:**\n",
    "        *   Parallelization (faster training).\n",
    "        *   Better at capturing long-range dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*   **Data Preparation:**\n",
    "    *   Crucial for LLM performance.\n",
    "    *   **Tokenization:** Breaking down text into tokens (words, subwords, characters).\n",
    "    *   **Byte Pair Encoding (BPE):** A common subword tokenization algorithm used in GPT-2.\n",
    "*   **Building the GPT-2 Architecture:**\n",
    "    *   Implementing the model in PyTorch (or another deep learning framework).\n",
    "    *   Understanding the role of each component (token embeddings, positional encodings, Transformer blocks, output layer).\n",
    "*   **Pre-training:**\n",
    "    *   Training on a massive general text dataset to learn broad language understanding.\n",
    "    *   Objective: Next-word prediction.\n",
    "    *   Loss function: Cross-entropy.\n",
    "*   **Fine-tuning:**\n",
    "    *   Adapting a pre-trained model to a specific task.\n",
    "    *   Training on a smaller, task-specific dataset.\n",
    "    *   **Instruction Fine-tuning:** Training the model to follow instructions.\n",
    "*   **Hugging Face `transformers` Library:**\n",
    "    *   Provides pre-trained models, tokenizers, and training utilities.\n",
    "    *   Simplifies the process of working with LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_prbnLeCshM",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **5.2 Limitations of this lecture**\n",
    "\n",
    "*   **Simplified Implementation:** We built a simplified version of GPT-2 for educational purposes. Real-world implementations are more complex and optimized.\n",
    "*   **Small Dataset:** We used a very small or dummy dataset for demonstration. Training state-of-the-art LLMs requires massive datasets.\n",
    "*   **Limited Compute:** We likely didn't have access to the computational resources (e.g., multiple high-end GPUs, TPUs) needed to train a large model from scratch.\n",
    "*   **Shallow Dive into Advanced Topics:** We touched upon advanced topics like model parallelism, mixed precision training, and optimized kernels, but didn't explore them in depth.\n",
    "*   **No Coverage of Evaluation Metrics:** We didn't cover metrics like perplexity, BLEU score, ROUGE, etc., which are essential for evaluating language models.\n",
    "*   **Ethical Considerations:** We only briefly mentioned the ethical implications of LLMs. A thorough discussion of bias, fairness, safety, and potential misuse is crucial when working with these powerful models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2xVmfaXDF6h",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Endnote\n",
    "\n",
    "We welcome comments and corrections. Please feel free to contact us at leslie@aisingapore.org and william@aisingapore.org."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-GcsISdDCuc",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### **5.3 Resources for Continued Learning**\n",
    "\n",
    "* **Building LLMs from Scratch** [ ]\n",
    "\n",
    "*   **Hugging Face:**\n",
    "    *   **Hugging Face Website:** [https://huggingface.co/](https://huggingface.co/)\n",
    "        *   A vast repository of pre-trained models, datasets, and code.\n",
    "        *   Explore different GPT-2 models, other LLMs, and various NLP tasks.\n",
    "    *   **Hugging Face `transformers` Documentation:** [https://huggingface.co/docs/transformers/index](https://huggingface.co/docs/transformers/index)\n",
    "        *   Comprehensive documentation on using the library.\n",
    "        *   Tutorials, examples, and API references.\n",
    "    *   **Hugging Face Course:** [https://huggingface.co/learn/nlp-course/](https://huggingface.co/learn/nlp-course/)\n",
    "        *   A free online course that covers many aspects of NLP, including Transformers and LLMs.\n",
    "*   **Research Papers:**\n",
    "    *   **\"Attention is All You Need\" (Original Transformer paper):** [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)\n",
    "    *   **\"Language Models are Unsupervised Multitask Learners\" (GPT-2 paper):** [https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)\n",
    "    *   **\"Scaling Laws for Neural Language Models\":** [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361)\n",
    "    *   **\"Training language models to follow instructions with human feedback\" (InstructGPT paper):** [https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)\n",
    "*   **Blogs and Articles:**\n",
    "    *   **The Illustrated Transformer (Jay Alammar):** [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)\n",
    "    *   **The Illustrated GPT-2 (Jay Alammar):** [https://jalammar.github.io/illustrated-gpt2/](https://jalammar.github.io/illustrated-gpt2/)\n",
    "    *   **Andrej Karpathy's blog:** [http://karpathy.github.io/](http://karpathy.github.io/) (Especially \"The Unreasonable Effectiveness of Recurrent Neural Networks\")\n",
    "    *   **Hugging Face Blog:** [https://huggingface.co/blog](https://huggingface.co/blog)\n",
    "*   **Courses:**\n",
    "    *   **Stanford CS224N: Natural Language Processing with Deep Learning:** [https://web.stanford.edu/class/cs224n/](https://web.stanford.edu/class/cs224n/)\n",
    "    *   **UC Berkeley CS294: Deep Reinforcement Learning:** [http://rail.eecs.berkeley.edu/deeprlcourse/](http://rail.eecs.berkeley.edu/deeprlcourse/) (Has lectures on advanced NLP topics)\n",
    "*   **Other Libraries and Frameworks:**\n",
    "    *   **Fairseq (Meta AI):** [https://fairseq.readthedocs.io/en/latest/](https://fairseq.readthedocs.io/en/latest/)\n",
    "    *   **TensorFlow/Keras:** [https://www.tensorflow.org/](https://www.tensorflow.org/)\n",
    "    *   **DeepSpeed (Microsoft):** [https://www.deepspeed.ai/](https://www.deepspeed.ai/) (For training very large models)\n",
    "    *   **Megatron-LM (NVIDIA):** [https://github.com/NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM) (For large-scale distributed training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsNm1ZL9cLJ6"
   },
   "outputs": [],
   "source": [
    "dd"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
